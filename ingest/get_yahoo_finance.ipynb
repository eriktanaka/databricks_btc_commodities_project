{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238d4124-ab7b-462e-ada9-091980dc9aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "-- Volume para zona raw (arquivos JSON)\n",
    "CREATE VOLUME IF NOT EXISTS lakehouse.raw_public.raw_yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd941ff-e431-494e-a450-9b8482a0c4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%python\n",
    "%pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b8f740-d35b-45a3-80db-c8f57fe7ac14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bb2038-573d-45e9-9378-4385428d2279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, UTC\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Caminho do Volume UC (Raw Zone)\n",
    "RAW_BASE_PATH = \"/Volumes/lakehouse/raw_public/raw_yfinance/commodities/latest_prices\"\n",
    "\n",
    "def get_commodities_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retorna as últimas cotações (1 minuto) de Ouro, Petróleo e Prata via Yahoo Finance.\n",
    "    \"\"\"\n",
    "    symbols = [\"GC=F\", \"CL=F\", \"SI=F\"]  # Ouro, Petróleo, Prata\n",
    "    dfs = []\n",
    "\n",
    "    for sym in symbols:\n",
    "        # Baixa o último preço (1 minuto)\n",
    "        try:\n",
    "            ultimo_df = yf.Ticker(sym).history(period=\"1d\", interval=\"1m\")[[\"Close\"]].tail(1)\n",
    "            if ultimo_df.empty:\n",
    "                continue\n",
    "\n",
    "            ultimo_df = ultimo_df.rename(columns={\"Close\": \"preco\"})\n",
    "            ultimo_df[\"ativo\"] = sym\n",
    "            ultimo_df[\"moeda\"] = \"USD\"\n",
    "            ultimo_df[\"horario_coleta\"] = datetime.now(UTC)\n",
    "\n",
    "            dfs.append(ultimo_df[[\"ativo\", \"preco\", \"moeda\", \"horario_coleta\"]])\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Erro ao buscar {sym}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        raise ValueError(\"Nenhuma cotação retornada pelo Yahoo Finance.\")\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Coleta\n",
    "pdf = get_commodities_df()\n",
    "\n",
    "# Define schema explícito (boa prática para ingestion)\n",
    "schema = StructType([\n",
    "    StructField(\"ativo\", StringType(), False),\n",
    "    StructField(\"preco\", DoubleType(), False),\n",
    "    StructField(\"moeda\", StringType(), False),\n",
    "    StructField(\"horario_coleta\", TimestampType(), False),\n",
    "])\n",
    "\n",
    "# Cria DataFrame Spark\n",
    "df = (\n",
    "    spark.createDataFrame(pdf, schema=schema)\n",
    "        .withColumn(\"ingestion_ts_utc\", F.current_timestamp())\n",
    "        .withColumn(\"source_system\", F.lit(\"yfinance\"))\n",
    "        .withColumn(\"source_endpoint\", F.lit(\"https://finance.yahoo.com\"))\n",
    "        .withColumn(\"ingestion_date\", F.to_date(F.col(\"ingestion_ts_utc\")))\n",
    ")\n",
    "\n",
    "# Escrita no Volume UC, particionada por ingestion_date\n",
    "(\n",
    "    df.write\n",
    "      .mode(\"append\")\n",
    "      .partitionBy(\"ingestion_date\")\n",
    "      .json(RAW_BASE_PATH)\n",
    ")\n",
    "\n",
    "print(\"✅ JSON salvo em:\", RAW_BASE_PATH)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8354050523679770,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "get_yahoo_finance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
